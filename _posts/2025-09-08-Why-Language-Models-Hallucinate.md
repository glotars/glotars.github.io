---
title: Почему языковые модели галлюцинируют
feed: show
date: 2025-09-08
---
Первым постом можно поговорить о наболевшем при построении AI-систем с LLM – галлюцинации моделей (или – генерацию выдуманного бреда).

На прошлой неделе исследователи из OpenAI опубликовали статью, которая так и называется: "[Why Language Models Hallucinate](https://www.arxiv.org/abs/2509.04664)". Если коротко – мы сами виноваты, потому что все бенчмарки для оценок LLM поощряют "угадывание" ответа, даже если модель его не знает, вместо честного "я не знаю", а модель не обучена проверять достоверность генерируемых текстовых цепочек. Исследователи приводят аналогию со студентом на экзамене, я могу привести аналогию с джуном на дейлике или с собой во время расследования инцидента на проде.

При этом даже изменение бенчмарков и поощрение неуверенности моделей вместо уверенного бреда не избавляет от проблемы – есть вещи, которых либо не могло быть в обучающих данных (об этом не написали в Интернете), либо которые встречались лишь один раз, а это тоже проблема.

Авторы назвали такие единичные факты "синглтонами" и приводят теорему с доказательством, что частота галлюцинаций на такие факты не может быть ниже, чем доля "синглтонов" в обучающем наборе данных. То есть, если 20% фактов о днях рождения встречаются в данных только один раз, то модель будет галлюцинировать как минимум на 20% таких запросов. Вот поэтому и не повайбкодишь на очередном модном JS-фреймворке, у которого 10 звёзд на гитхабе.

Кроме этого – есть доля бреда в самих обучающих данных (обучаемся на всём Интернете, поэтому вспомните хотя бы ответы мейл ру или реддит) и есть вычислительно-сложные задачи, на которые модель не сможет дать ответ (например, посчитать хэш-сумму бинарного файла).

Всё же авторы предлагают начать с того, чтобы штрафовать за уверенные неправильные ответы моделей на бенчмарках сильнее, чем за отсутствие ответа вообще в случае неуверенности самой модели. Ответа, как обучить модели, чтобы они не выдавали в 99%+ случаев "я не знаю", в статье нет, поэтому это видимо "задание на дом" для читателей.

Моё мнение – поскольку стопроцентного качества классификации на уверенность у моделей никогда не будут (и авторы сами об этом заявляют), то непонятно имеет ли вообще всё это смысл. В лучшем случае нас ждёт равная борьба "average фантазёров" и "да-она-же-ничего-не-знает enjoy'еров", потому что для многих LLM-приложений одинаково бесполезно, например, что вызовы выдуманных функций при вайб-кодинге, что простое "я ничего в этом не понимаю".

Вопрос чату – а как вам кажется, лучше пусть модели и дальше генерируют бред, если не знают правильный ответ, или лучше получать честное "я не могу ответить на Ваш вопрос", даже если иногда это будет ложноположительным срабатыванием?